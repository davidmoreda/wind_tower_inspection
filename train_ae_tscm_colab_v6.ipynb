{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# AE-TSCM + VQVAE v6 — Mayor capacidad + Codebook estable\n\nPipeline de fusión multi-iluminación para detección de defectos superficiales.\n\n**Pipeline:** 5 luces (SUP_IZQ, SUP_DER, INF_DER, INF_IZQ, ALL) → AE-TSCM → RGB → VQVAE → 5 luces reconstruidas\n\n**Cambios vs v5:**\n- ✅ **SIN CosineAnnealingLR** — learning rate constante 1e-3\n- ✅ **Mayor capacidad** — `hidden_channels=128`, `num_residual=4` (~8.5M params vs 2.1M)\n- ✅ **Codebook EMA** — actualización por media móvil exponencial (sin gradientes), más estable\n- ✅ **Codebook reset** — reemplaza entradas no usadas cada N épocas, evita codebook collapse\n- ✅ **Codebook más pequeño** — 256 entradas (vs 512), más fácil de utilizar completamente\n- ✅ **Optimizaciones de memoria** — `pin_memory=False`, `num_workers` reducidos, limpieza de caché CUDA\n- Resolución completa sin resize (5472×3072)\n- Encoder/Decoder con 4 etapas de downsampling (16x)\n- Mixed precision (AMP) + gradient accumulation\n- Requiere A100 (40GB+)\n\n---"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pytorch-msssim -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import os\nimport csv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\n# NO importamos scheduler — el paper usa LR constante\nfrom torch.amp import autocast, GradScaler\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom pytorch_msssim import ssim\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Dispositivo: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'VRAM: {vram_gb:.1f} GB')\n    print(f'VRAM disponible: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1e9:.1f} GB')\n    if vram_gb < 35:\n        print('⚠️  ADVERTENCIA: Este notebook requiere A100 (40GB+). Con menos VRAM puede fallar.')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Montar Google Drive y configurar rutas\n",
                "\n",
                "Sube la carpeta `MT001/` a tu Google Drive, por ejemplo en `MyDrive/dataset/MT001/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# ============================================================\n",
                "# AJUSTA ESTA RUTA a donde subiste la carpeta MT001\n",
                "# ============================================================\n",
                "DATASET_DIR = '/content/drive/MyDrive/dataset/MT001'\n",
                "\n",
                "# Verificar que existe\n",
                "assert os.path.isdir(DATASET_DIR), f'No se encuentra: {DATASET_DIR}'\n",
                "num_files = len([f for f in os.listdir(DATASET_DIR) if f.endswith('.jpg')])\n",
                "print(f'Encontradas {num_files} imágenes en {DATASET_DIR}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuración"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "CONFIG = {\n    # Datos\n    'data_dir': DATASET_DIR,\n    'num_lights': 5,              # SUP_IZQ, SUP_DER, INF_DER, INF_IZQ, ALL\n    'batch_size': 1,              # Batch size 1 para resolución completa\n    'gradient_accumulation_steps': 4,  # Simula batch efectivo de 4\n\n    # Modelo AE-TSCM\n    'use_spatial_attention': True,\n\n    # Modelo VQVAE — mayor capacidad para A100\n    'hidden_channels': 128,       # 128 vs 64 en v5 → más capacidad en encoder/decoder\n    'num_residual': 4,            # 4 bloques residuales vs 2 en v5\n    'num_embeddings': 256,        # 256 vs 512 → codebook más pequeño, más fácil de utilizar\n    'embedding_dim': 128,         # 128 vs 64 → latent más expresivo\n\n    # Codebook EMA\n    'ema_decay': 0.99,            # Factor de decaimiento para EMA\n    'commitment_cost': 0.25,      # Peso del commitment loss\n    'codebook_reset_interval': 20,  # Reset entradas no usadas cada N épocas\n    'codebook_usage_threshold': 2,  # Mínimo de usos para no ser reseteada\n\n    # Entrenamiento\n    'epochs': 150,\n    'learning_rate': 1e-3,        # LR constante, sin scheduler\n    'weight_decay': 1e-5,\n\n    # Pesos de pérdidas\n    'lambda_mse': 1.0,\n    'lambda_ssim': 0.05,\n    'lambda_vq': 0.02,\n\n    # Guardado\n    'save_interval': 25,\n    'log_interval': 10,\n}\n\n# Directorio de salida en Drive para no perder resultados\nSAVE_DIR = '/content/drive/MyDrive/runs/ae_tscm_v6_' + datetime.now().strftime('%Y%m%d_%H%M%S')\nos.makedirs(SAVE_DIR, exist_ok=True)\nos.makedirs(os.path.join(SAVE_DIR, 'checkpoints'), exist_ok=True)\nos.makedirs(os.path.join(SAVE_DIR, 'visualizations'), exist_ok=True)\nprint(f'Resultados se guardarán en: {SAVE_DIR}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset — Loader para formato MT001"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Orden fijo de las luces\nLIGHT_NAMES = ['SUP_IZQ', 'SUP_DER', 'INF_DER', 'INF_IZQ', 'ALL']\n\n\nclass CropToDivisible:\n    \"\"\"Recorta la imagen para que H y W sean divisibles por un factor (16 para 4 etapas de stride 2).\"\"\"\n    def __init__(self, factor=16):\n        self.factor = factor\n\n    def __call__(self, img):\n        _, h, w = img.shape\n        new_h = (h // self.factor) * self.factor\n        new_w = (w // self.factor) * self.factor\n        top = (h - new_h) // 2\n        left = (w - new_w) // 2\n        return img[:, top:top + new_h, left:left + new_w]\n\n\nclass MT001Dataset(Dataset):\n    \"\"\"\n    Dataset para imágenes MT001 con 5 iluminaciones.\n    Agrupa por num_captura y devuelve (5, H, W) en escala de grises.\n    Trabaja a resolución completa (sin resize).\n    \"\"\"\n\n    def __init__(self, root_dir, light_names=LIGHT_NAMES, transform=None, captures=None):\n        self.root_dir = root_dir\n        self.light_names = light_names\n        self.num_lights = len(light_names)\n        self.transform = transform\n        self.groups = self._parse_metadata(captures)\n        print(f'  Dataset: {len(self.groups)} capturas, {self.num_lights} luces cada una')\n\n    def _parse_metadata(self, captures):\n        metadata_path = os.path.join(self.root_dir, 'metadata.csv')\n        groups = defaultdict(dict)\n\n        with open(metadata_path, 'r') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                cap = int(row['num_captura'])\n                luz = row['luz']\n                if luz in self.light_names:\n                    groups[cap][luz] = row['imagen']\n\n        valid = []\n        for cap in sorted(groups.keys()):\n            if captures is not None and cap not in captures:\n                continue\n            if len(groups[cap]) == self.num_lights:\n                valid.append((cap, groups[cap]))\n\n        return valid\n\n    def __len__(self):\n        return len(self.groups)\n\n    def __getitem__(self, idx):\n        cap_num, filenames = self.groups[idx]\n        channels = []\n\n        for light_name in self.light_names:\n            img_path = os.path.join(self.root_dir, filenames[light_name])\n            image = Image.open(img_path).convert('L')  # Escala de grises\n            image = transforms.ToTensor()(image)  # (1, H, W), rango [0,1]\n            channels.append(image)\n\n        multi_channel = torch.cat(channels, dim=0)  # (5, H, W)\n\n        if self.transform:\n            multi_channel = self.transform(multi_channel)\n\n        return multi_channel\n\n\ndef get_train_transforms():\n    \"\"\"Transforms para entrenamiento a resolución completa.\"\"\"\n    return transforms.Compose([\n        CropToDivisible(16),\n        transforms.RandomHorizontalFlip(p=0.5),\n    ])\n\n\ndef get_val_transforms():\n    \"\"\"Transforms para validación a resolución completa.\"\"\"\n    return transforms.Compose([\n        CropToDivisible(16),\n    ])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Crear dataset con TODAS las capturas (sin split, como el paper)\ntrain_dataset = MT001Dataset(\n    CONFIG['data_dir'],\n    transform=get_train_transforms(),\n)\n\n# OPTIMIZACIÓN: pin_memory=False para evitar OOM, num_workers reducido\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=CONFIG['batch_size'], \n    shuffle=True,\n    num_workers=0,         # Reducido a 0 para minimizar uso de memoria\n    pin_memory=False,      # Desactivado para evitar OOM en GPU\n    persistent_workers=False\n)\n\n# Verificar una muestra\nsample = train_dataset[0]\nprint(f'\\nForma muestra: {sample.shape}')  # (5, H, W) a resolución completa\nprint(f'Rango valores: [{sample.min():.3f}, {sample.max():.3f}]')\nprint(f'Total capturas para entrenamiento: {len(train_dataset)}')\n\n# Verificar memoria disponible\nif torch.cuda.is_available():\n    print(f'\\nMemoria GPU:')\n    print(f'  Asignada: {torch.cuda.memory_allocated(0)/1e9:.2f} GB')\n    print(f'  Reservada: {torch.cuda.memory_reserved(0)/1e9:.2f} GB')\n    print(f'  Disponible: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1e9:.2f} GB')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Modelos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# AE-TSCM: Attention-Enhanced Taylor Series Channel Mixer\n",
                "# ============================================================\n",
                "\n",
                "class ChannelAttention(nn.Module):\n",
                "    \"\"\"SE-Net: aprende qué luces son más importantes.\"\"\"\n",
                "    def __init__(self, num_channels, reduction=2):\n",
                "        super().__init__()\n",
                "        hidden = max(num_channels // reduction, 4)\n",
                "        self.attention = nn.Sequential(\n",
                "            nn.Linear(num_channels, hidden),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden, num_channels),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        b, c, _, _ = x.shape\n",
                "        gap = x.mean(dim=[2, 3])\n",
                "        weights = self.attention(gap)\n",
                "        return weights.view(b, c, 1, 1)\n",
                "\n",
                "\n",
                "class SpatialAttention(nn.Module):\n",
                "    \"\"\"CBAM: aprende qué regiones espaciales son más importantes.\"\"\"\n",
                "    def __init__(self, kernel_size=7):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        avg_out = x.mean(dim=1, keepdim=True)\n",
                "        max_out, _ = x.max(dim=1, keepdim=True)\n",
                "        return self.conv(torch.cat([avg_out, max_out], dim=1))\n",
                "\n",
                "\n",
                "class TaylorTransform(nn.Module):\n",
                "    \"\"\"Transformación no lineal de intensidad con coeficientes aprendibles.\"\"\"\n",
                "    def __init__(self, num_coefficients=5):\n",
                "        super().__init__()\n",
                "        initial = torch.zeros(num_coefficients)\n",
                "        initial[1] = 1.0  # f(x) = x inicialmente\n",
                "        self.coefficients = nn.Parameter(initial)\n",
                "\n",
                "    def forward(self, x):\n",
                "        c = self.coefficients\n",
                "        return (c[0] + c[1]*x + c[2]*(x**2)/2 + c[3]*(x**3)/6 + c[4]*(x**4)/24)\n",
                "\n",
                "\n",
                "class ChannelMixer(nn.Module):\n",
                "    \"\"\"Mezcla N canales a M canales con conv 1x1.\"\"\"\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super().__init__()\n",
                "        self.mixer = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.LeakyReLU(0.1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.mixer(x)\n",
                "\n",
                "\n",
                "class ChannelNormalize(nn.Module):\n",
                "    \"\"\"Normaliza cada canal a [0, 1].\"\"\"\n",
                "    def forward(self, x):\n",
                "        b, c, h, w = x.shape\n",
                "        x_flat = x.view(b, c, -1)\n",
                "        mn = x_flat.min(dim=2, keepdim=True)[0].unsqueeze(-1)\n",
                "        mx = x_flat.max(dim=2, keepdim=True)[0].unsqueeze(-1)\n",
                "        return (x - mn) / (mx - mn + 1e-8)\n",
                "\n",
                "\n",
                "class AE_TSCM(nn.Module):\n",
                "    \"\"\"5 luces → 3 canales RGB.\"\"\"\n",
                "    def __init__(self, in_channels=5, out_channels=3, use_spatial_attention=True):\n",
                "        super().__init__()\n",
                "        self.use_spatial_attention = use_spatial_attention\n",
                "        self.taylor = TaylorTransform(5)\n",
                "        self.channel_attention = ChannelAttention(in_channels)\n",
                "        if use_spatial_attention:\n",
                "            self.spatial_attention = SpatialAttention(7)\n",
                "        self.mixer = ChannelMixer(in_channels, out_channels)\n",
                "        self.normalize = ChannelNormalize()\n",
                "\n",
                "    def forward(self, x, return_attention=False):\n",
                "        x = self.taylor(x)\n",
                "        ch_w = self.channel_attention(x)\n",
                "        x = x * ch_w\n",
                "        sp_w = None\n",
                "        if self.use_spatial_attention:\n",
                "            sp_w = self.spatial_attention(x)\n",
                "            x = x * sp_w\n",
                "        x = self.mixer(x)\n",
                "        x = self.normalize(x)\n",
                "        if return_attention:\n",
                "            return x, ch_w.squeeze(), sp_w\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# ============================================================\n# VQVAE: Vector Quantized VAE — 4 etapas (16x down/up)\n# Con EMA codebook + reset de entradas no usadas\n# ============================================================\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(channels, channels, 3, padding=1)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass Encoder(nn.Module):\n    \"\"\"4 etapas de downsampling (16x) para resolución completa.\"\"\"\n    def __init__(self, in_channels=3, hidden_channels=128, embedding_dim=128, num_residual=4):\n        super().__init__()\n        hc = hidden_channels\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, hc, 4, stride=2, padding=1), nn.ReLU())       # /2\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(hc, hc, 4, stride=2, padding=1), nn.ReLU())                # /4\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(hc, hc * 2, 4, stride=2, padding=1), nn.ReLU())            # /8\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(hc * 2, hc * 2, 4, stride=2, padding=1), nn.ReLU())        # /16\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(hc * 2) for _ in range(num_residual)])\n        self.to_latent = nn.Conv2d(hc * 2, embedding_dim, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.residual_blocks(x)\n        return self.to_latent(x)\n\n\nclass VectorQuantizerEMA(nn.Module):\n    \"\"\"\n    VQ con actualización EMA del codebook (más estable que gradientes).\n    Incluye reset de entradas no usadas para evitar codebook collapse.\n    \"\"\"\n    def __init__(self, num_embeddings=256, embedding_dim=128,\n                 commitment_cost=0.25, decay=0.99, epsilon=1e-5):\n        super().__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.commitment_cost = commitment_cost\n        self.decay = decay\n        self.epsilon = epsilon\n\n        # Codebook — inicialización uniforme\n        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n        self.codebook.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n        self.codebook.weight.requires_grad = False  # EMA actualiza, no gradientes\n\n        # Buffers EMA (no son parámetros del optimizador)\n        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))\n        self.register_buffer('ema_dw', self.codebook.weight.data.clone())\n        self.register_buffer('usage_count', torch.zeros(num_embeddings))\n\n    def forward(self, z):\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_shape = z.shape\n        z_flat = z.view(-1, self.embedding_dim)\n\n        # Distancias al codebook\n        distances = (\n            torch.sum(z_flat**2, dim=1, keepdim=True) +\n            torch.sum(self.codebook.weight**2, dim=1) -\n            2 * torch.matmul(z_flat, self.codebook.weight.t())\n        )\n\n        encoding_indices = torch.argmin(distances, dim=1)\n        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n        quantized = self.codebook(encoding_indices).view(z_shape)\n\n        # EMA update del codebook (solo en training)\n        if self.training:\n            encodings_sum = encodings.sum(0)\n            dw = encodings.t() @ z_flat\n\n            self.ema_cluster_size.data.mul_(self.decay).add_(\n                encodings_sum, alpha=1 - self.decay)\n            self.ema_dw.data.mul_(self.decay).add_(\n                dw, alpha=1 - self.decay)\n\n            # Laplace smoothing para evitar divisiones por cero\n            n = self.ema_cluster_size.sum()\n            cluster_size = (\n                (self.ema_cluster_size + self.epsilon) /\n                (n + self.num_embeddings * self.epsilon) * n\n            )\n\n            self.codebook.weight.data.copy_(\n                self.ema_dw / cluster_size.unsqueeze(1))\n\n            # Acumular uso para reset\n            self.usage_count.add_(encodings_sum)\n\n        # Solo commitment loss (codebook se actualiza por EMA, no por gradientes)\n        commitment_loss = F.mse_loss(z, quantized.detach())\n        loss = self.commitment_cost * commitment_loss\n\n        # Straight-through estimator\n        quantized = z + (quantized - z).detach()\n\n        # Perplexity (mide utilización del codebook)\n        avg_probs = encodings.mean(0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n        return quantized, loss, perplexity\n\n    def reset_unused_codes(self, z_flat_samples, threshold=2):\n        \"\"\"\n        Reemplaza entradas del codebook con uso < threshold\n        por vectores aleatorios del encoder (z_flat_samples).\n        \"\"\"\n        unused_mask = self.usage_count < threshold\n        num_unused = unused_mask.sum().item()\n\n        if num_unused > 0 and z_flat_samples.shape[0] > 0:\n            n_replace = min(int(num_unused), z_flat_samples.shape[0])\n            # Muestrear vectores del encoder para reemplazar entradas muertas\n            perm = torch.randperm(z_flat_samples.shape[0], device=z_flat_samples.device)\n            replace_vectors = z_flat_samples[perm[:n_replace]].detach()\n\n            unused_indices = torch.where(unused_mask)[0][:n_replace]\n            self.codebook.weight.data[unused_indices] = replace_vectors\n            self.ema_dw.data[unused_indices] = replace_vectors\n            self.ema_cluster_size.data[unused_indices] = 1.0\n\n        # Reset contador de uso para el siguiente periodo\n        self.usage_count.zero_()\n        return num_unused\n\n\nclass Decoder(nn.Module):\n    \"\"\"4 etapas de upsampling (16x) para resolución completa.\"\"\"\n    def __init__(self, out_channels=5, hidden_channels=128, embedding_dim=128, num_residual=4):\n        super().__init__()\n        hc = hidden_channels\n        self.from_latent = nn.Conv2d(embedding_dim, hc * 2, 1)\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(hc * 2) for _ in range(num_residual)])\n        self.deconv1 = nn.Sequential(\n            nn.ConvTranspose2d(hc * 2, hc * 2, 4, stride=2, padding=1), nn.ReLU())   # x2\n        self.deconv2 = nn.Sequential(\n            nn.ConvTranspose2d(hc * 2, hc, 4, stride=2, padding=1), nn.ReLU())       # x4\n        self.deconv3 = nn.Sequential(\n            nn.ConvTranspose2d(hc, hc, 4, stride=2, padding=1), nn.ReLU())           # x8\n        self.deconv4 = nn.Sequential(\n            nn.ConvTranspose2d(hc, out_channels, 4, stride=2, padding=1), nn.Sigmoid())  # x16\n\n    def forward(self, z):\n        x = self.from_latent(z)\n        x = self.residual_blocks(x)\n        x = self.deconv1(x)\n        x = self.deconv2(x)\n        x = self.deconv3(x)\n        return self.deconv4(x)\n\n\nclass VQVAE(nn.Module):\n    def __init__(self, in_channels=3, out_channels=5, hidden_channels=128,\n                 num_residual=4, num_embeddings=256, embedding_dim=128,\n                 commitment_cost=0.25, ema_decay=0.99):\n        super().__init__()\n        self.encoder = Encoder(in_channels, hidden_channels, embedding_dim, num_residual)\n        self.vq = VectorQuantizerEMA(num_embeddings, embedding_dim, commitment_cost, ema_decay)\n        self.decoder = Decoder(out_channels, hidden_channels, embedding_dim, num_residual)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        z_q, vq_loss, perplexity = self.vq(z)\n        return self.decoder(z_q), vq_loss, perplexity\n\n    def encode(self, x):\n        z = self.encoder(x)\n        z_q, _, _ = self.vq(z)\n        return z_q"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# ============================================================\n# Modelo Completo: AE-TSCM + VQVAE\n# ============================================================\n\nclass FullModel(nn.Module):\n    \"\"\"5 luces → AE-TSCM → RGB → VQVAE → 5 luces reconstruidas\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.ae_tscm = AE_TSCM(\n            in_channels=config['num_lights'],\n            out_channels=3,\n            use_spatial_attention=config['use_spatial_attention']\n        )\n        self.vqvae = VQVAE(\n            in_channels=3,\n            out_channels=config['num_lights'],\n            hidden_channels=config['hidden_channels'],\n            num_residual=config['num_residual'],\n            num_embeddings=config['num_embeddings'],\n            embedding_dim=config['embedding_dim'],\n            commitment_cost=config['commitment_cost'],\n            ema_decay=config['ema_decay']\n        )\n\n    def forward(self, x, return_intermediate=False):\n        if return_intermediate:\n            rgb, ch_att, sp_att = self.ae_tscm(x, return_attention=True)\n        else:\n            rgb = self.ae_tscm(x, return_attention=False)\n        reconstructed, vq_loss, perplexity = self.vqvae(rgb)\n        if return_intermediate:\n            return reconstructed, vq_loss, perplexity, rgb, ch_att, sp_att\n        return reconstructed, vq_loss, perplexity\n\n\n# Crear modelo y mostrar parámetros\nmodel = FullModel(CONFIG).to(device)\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Parámetros totales: {total_params:,}')\nprint(f'Parámetros entrenables: {trainable_params:,}  (codebook se actualiza por EMA)')\nprint(f'Codebook: {CONFIG[\"num_embeddings\"]} entradas × {CONFIG[\"embedding_dim\"]}d')\n\n# Verificar memoria después de crear modelo\nif torch.cuda.is_available():\n    print(f'\\nMemoria GPU después de cargar modelo:')\n    print(f'  Asignada: {torch.cuda.memory_allocated(0)/1e9:.2f} GB')\n    print(f'  Reservada: {torch.cuda.memory_reserved(0)/1e9:.2f} GB')\n    print(f'  Disponible: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1e9:.2f} GB')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Funciones de pérdida y visualización"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def compute_loss(original, reconstructed, vq_loss, config):\n    mse_loss = F.mse_loss(reconstructed, original)\n    ssim_loss = 1 - ssim(reconstructed, original, data_range=1.0, size_average=True)\n    total = (config['lambda_mse'] * mse_loss +\n             config['lambda_ssim'] * ssim_loss +\n             config['lambda_vq'] * vq_loss)\n    return total, {\n        'total': total.item(),\n        'mse': mse_loss.item(),\n        'ssim': ssim_loss.item(),\n        'vq': vq_loss.item()\n    }\n\n\ndef save_visualization(original, reconstructed, rgb, epoch, save_dir):\n    original = original[0].detach().cpu()\n    reconstructed = reconstructed[0].detach().cpu()\n    rgb = rgb[0].detach().cpu()\n    num_lights = original.shape[0]\n\n    fig, axes = plt.subplots(3, max(num_lights, 3), figsize=(3*num_lights, 9))\n\n    for i in range(num_lights):\n        axes[0, i].imshow(original[i], cmap='gray', vmin=0, vmax=1)\n        axes[0, i].set_title(f'Orig {LIGHT_NAMES[i]}', fontsize=8)\n        axes[0, i].axis('off')\n\n        axes[1, i].imshow(reconstructed[i], cmap='gray', vmin=0, vmax=1)\n        axes[1, i].set_title(f'Recon {LIGHT_NAMES[i]}', fontsize=8)\n        axes[1, i].axis('off')\n\n    rgb_img = rgb.permute(1, 2, 0).numpy()\n    rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min() + 1e-8)\n    axes[2, 0].imshow(rgb_img)\n    axes[2, 0].set_title('RGB Fusionado')\n    axes[2, 0].axis('off')\n    for i in range(1, max(num_lights, 3)):\n        axes[2, i].axis('off')\n\n    plt.suptitle(f'Época {epoch}', fontsize=14)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:04d}.png'), dpi=100)\n    plt.show()\n    plt.close()\n\n\ndef plot_losses(history, save_dir):\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n    epochs = range(1, len(history['total']) + 1)\n\n    for ax, key, title in zip(\n        axes.flat[:4],\n        ['total', 'mse', 'ssim', 'vq'],\n        ['Total Loss', 'MSE Loss', 'SSIM Loss', 'VQ Loss']\n    ):\n        ax.plot(epochs, history[key], label='train')\n        ax.set_title(title)\n        ax.set_xlabel('Epoch')\n        ax.legend()\n\n    # Perplexity (utilización del codebook)\n    ax_perp = axes[1, 1]\n    ax_perp.plot(epochs, history['perplexity'], label='perplexity', color='green')\n    ax_perp.axhline(y=CONFIG['num_embeddings'], color='r', linestyle='--', alpha=0.5,\n                    label=f'max ({CONFIG[\"num_embeddings\"]})')\n    ax_perp.set_title('Codebook Perplexity')\n    ax_perp.set_xlabel('Epoch')\n    ax_perp.legend()\n\n    # SSIM value (1 - ssim_loss)\n    ax_ssim_val = axes[1, 2]\n    ssim_values = [1 - s for s in history['ssim']]\n    ax_ssim_val.plot(epochs, ssim_values, label='SSIM', color='purple')\n    ax_ssim_val.set_title('SSIM (calidad reconstrucción)')\n    ax_ssim_val.set_xlabel('Epoch')\n    ax_ssim_val.set_ylim(0, 1)\n    ax_ssim_val.legend()\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, 'loss_history.png'), dpi=150)\n    plt.show()\n    plt.close()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Entrenamiento"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def run_epoch(model, dataloader, optimizer, config, scaler=None, train=True):\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    total_losses = {'total': 0, 'mse': 0, 'ssim': 0, 'vq': 0}\n    total_perplexity = 0\n    num_batches = 0\n    accum_steps = config.get('gradient_accumulation_steps', 1)\n    use_amp = scaler is not None\n\n    ctx = torch.no_grad() if not train else torch.enable_grad()\n    with ctx:\n        if train:\n            optimizer.zero_grad()\n        for i, batch in enumerate(tqdm(dataloader, desc='Train', leave=False)):\n            batch = batch.to(device)\n\n            with autocast('cuda', enabled=use_amp):\n                reconstructed, vq_loss, perplexity = model(batch)\n                loss, loss_dict = compute_loss(batch, reconstructed, vq_loss, config)\n                if train:\n                    loss = loss / accum_steps\n\n            if train:\n                if use_amp:\n                    scaler.scale(loss).backward()\n                    if (i + 1) % accum_steps == 0:\n                        scaler.step(optimizer)\n                        scaler.update()\n                        optimizer.zero_grad()\n                else:\n                    loss.backward()\n                    if (i + 1) % accum_steps == 0:\n                        optimizer.step()\n                        optimizer.zero_grad()\n\n            for k in total_losses:\n                total_losses[k] += loss_dict[k]\n            total_perplexity += perplexity.item()\n            num_batches += 1\n\n        # Flush gradientes restantes si no son divisibles por accum_steps\n        if train and num_batches % accum_steps != 0:\n            if use_amp:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n\n    for k in total_losses:\n        total_losses[k] /= max(num_batches, 1)\n    avg_perplexity = total_perplexity / max(num_batches, 1)\n    return total_losses, avg_perplexity\n\n\ndef collect_encoder_outputs(model, dataloader, max_samples=8):\n    \"\"\"Recoge outputs del encoder para usar como reemplazos en codebook reset.\"\"\"\n    model.eval()\n    z_samples = []\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            if i >= max_samples:\n                break\n            batch = batch.to(device)\n            with autocast('cuda'):\n                rgb = model.ae_tscm(batch)\n                z = model.vqvae.encoder(rgb)\n            # Flatten spatial dims: (B, D, H, W) → (B*H*W, D)\n            z_flat = z.permute(0, 2, 3, 1).contiguous().view(-1, z.shape[1])\n            z_samples.append(z_flat.float())\n    if z_samples:\n        return torch.cat(z_samples, dim=0)\n    return torch.empty(0)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# ============================================================\n# Optimizador y scaler (AMP) — SIN SCHEDULER, LR constante\n# ============================================================\noptimizer = Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\nscaler = GradScaler('cuda')\n\n# Historial\nhistory = {k: [] for k in ['total', 'mse', 'ssim', 'vq', 'perplexity']}\n\nprint(f'Iniciando entrenamiento: {CONFIG[\"epochs\"]} épocas')\nprint(f'Muestras: {len(train_dataset)} (todas, sin split)')\nprint(f'Batch size: {CONFIG[\"batch_size\"]} | Gradient accumulation: {CONFIG[\"gradient_accumulation_steps\"]} (batch efectivo: {CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]})')\nprint(f'Learning rate: {CONFIG[\"learning_rate\"]} (CONSTANTE, sin scheduler)')\nprint(f'Mixed precision (AMP): activado')\nprint(f'Codebook: {CONFIG[\"num_embeddings\"]} entradas, EMA decay={CONFIG[\"ema_decay\"]}, reset cada {CONFIG[\"codebook_reset_interval\"]} épocas')\nprint(f'⚡ Optimización de memoria: pin_memory=False, num_workers=0')\nprint('=' * 70)\n\nfor epoch in range(1, CONFIG['epochs'] + 1):\n    # Limpiar caché CUDA al inicio de cada época\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Train\n    train_losses, avg_perplexity = run_epoch(model, train_loader, optimizer, CONFIG, scaler=scaler, train=True)\n\n    # Guardar historial\n    for k in ['total', 'mse', 'ssim', 'vq']:\n        history[k].append(train_losses[k])\n    history['perplexity'].append(avg_perplexity)\n\n    # Log\n    print(f'Ep {epoch:3d}/{CONFIG[\"epochs\"]} | '\n          f'Loss: {train_losses[\"total\"]:.4f} (mse:{train_losses[\"mse\"]:.4f} ssim:{train_losses[\"ssim\"]:.4f} vq:{train_losses[\"vq\"]:.4f}) | '\n          f'Perp: {avg_perplexity:.1f}/{CONFIG[\"num_embeddings\"]}')\n    \n    # Mostrar memoria cada 10 épocas\n    if epoch % 10 == 0 and torch.cuda.is_available():\n        print(f'  >> Mem: {torch.cuda.memory_allocated(0)/1e9:.2f} GB asignada, '\n              f'{torch.cuda.max_memory_allocated(0)/1e9:.2f} GB máx')\n\n    # Codebook reset — reemplaza entradas no usadas con outputs reales del encoder\n    if epoch % CONFIG['codebook_reset_interval'] == 0:\n        z_samples = collect_encoder_outputs(model, train_loader, max_samples=8)\n        num_reset = model.vqvae.vq.reset_unused_codes(\n            z_samples, threshold=CONFIG['codebook_usage_threshold'])\n        usage_pct = (CONFIG['num_embeddings'] - num_reset) / CONFIG['num_embeddings'] * 100\n        print(f'  >> Codebook reset: {num_reset} entradas reemplazadas, '\n              f'utilización: {usage_pct:.0f}%')\n\n    # Visualización\n    if epoch % CONFIG['log_interval'] == 0 or epoch == 1:\n        model.eval()\n        with torch.no_grad():\n            sample = next(iter(train_loader)).to(device)\n            with autocast('cuda'):\n                recon, _, _, rgb, _, _ = model(sample, return_intermediate=True)\n            save_visualization(sample, recon.float(), rgb.float(), epoch,\n                               os.path.join(SAVE_DIR, 'visualizations'))\n\n    # Checkpoint\n    if epoch % CONFIG['save_interval'] == 0:\n        ckpt_path = os.path.join(SAVE_DIR, 'checkpoints', f'ckpt_epoch_{epoch:04d}.pt')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scaler_state_dict': scaler.state_dict(),\n            'train_loss': train_losses['total'],\n            'perplexity': avg_perplexity,\n            'config': CONFIG\n        }, ckpt_path)\n        print(f'  >> Checkpoint guardado: epoch {epoch}')\n\n# Guardar modelo final y gráficas\ntorch.save(model.state_dict(), os.path.join(SAVE_DIR, 'final_model.pt'))\nplot_losses(history, SAVE_DIR)\n\nprint('\\n' + '=' * 70)\nprint(f'Entrenamiento completado. Loss final: {train_losses[\"total\"]:.4f}')\nprint(f'Perplexity final: {avg_perplexity:.1f}/{CONFIG[\"num_embeddings\"]}')\nprint(f'Resultados en: {SAVE_DIR}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluación final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Cargar modelo final y visualizar reconstrucciones\nmodel.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'final_model.pt')))\nmodel.eval()\n\nprint('Visualización de muestras con el modelo final:\\n')\n\n# Métricas finales agregadas\nall_mse = []\nall_ssim = []\n\nwith torch.no_grad():\n    for i, batch in enumerate(train_loader):\n        batch = batch.to(device)\n        with autocast('cuda'):\n            recon, vq_loss, perp, rgb, ch_att, sp_att = model(batch, return_intermediate=True)\n\n        # Métricas por muestra\n        mse_val = F.mse_loss(recon.float(), batch).item()\n        ssim_val = ssim(recon.float(), batch, data_range=1.0, size_average=True).item()\n        all_mse.append(mse_val)\n        all_ssim.append(ssim_val)\n\n        if i < 3:  # Solo mostrar 3 muestras detalladas\n            # Channel attention weights\n            ch_weights = ch_att.float().cpu().numpy().flatten()\n            print(f'Muestra {i} — MSE: {mse_val:.4f}, SSIM: {ssim_val:.4f}')\n            print(f'  Channel Attention weights:')\n            for j, name in enumerate(LIGHT_NAMES):\n                bar = '█' * int(ch_weights[j] * 30)\n                print(f'    {name:8s}: {ch_weights[j]:.3f} {bar}')\n            print()\n\n            # Visualizar\n            orig = batch[0].cpu()\n            rec = recon[0].float().cpu()\n\n            fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n            for j in range(5):\n                axes[0, j].imshow(orig[j], cmap='gray', vmin=0, vmax=1)\n                axes[0, j].set_title(f'Orig {LIGHT_NAMES[j]}', fontsize=9)\n                axes[0, j].axis('off')\n                axes[1, j].imshow(rec[j], cmap='gray', vmin=0, vmax=1)\n                axes[1, j].set_title(f'Recon {LIGHT_NAMES[j]}', fontsize=9)\n                axes[1, j].axis('off')\n            plt.suptitle(f'Muestra {i} — MSE: {mse_val:.4f}, SSIM: {ssim_val:.4f}')\n            plt.tight_layout()\n            plt.show()\n\n# Resumen\nprint('=' * 50)\nprint(f'Métricas finales sobre {len(all_mse)} muestras:')\nprint(f'  MSE  medio: {sum(all_mse)/len(all_mse):.4f}')\nprint(f'  SSIM medio: {sum(all_ssim)/len(all_ssim):.4f}')\nprint(f'  Perplexity final: {history[\"perplexity\"][-1]:.1f}/{CONFIG[\"num_embeddings\"]}')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Mostrar spatial attention del modelo final\nwith torch.no_grad():\n    sample = next(iter(train_loader)).to(device)\n    with autocast('cuda'):\n        _, _, _, rgb, _, sp_att = model(sample, return_intermediate=True)\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Spatial attention map\n    sp_map = sp_att[0, 0].float().cpu().numpy()\n    axes[0].imshow(sp_map, cmap='hot')\n    axes[0].set_title('Spatial Attention (zonas importantes)')\n    axes[0].axis('off')\n\n    # RGB fusionado\n    rgb_np = rgb[0].float().cpu().permute(1, 2, 0).numpy()\n    rgb_np = (rgb_np - rgb_np.min()) / (rgb_np.max() - rgb_np.min() + 1e-8)\n    axes[1].imshow(rgb_np)\n    axes[1].set_title('RGB Fusionado')\n    axes[1].axis('off')\n\n    # Original SUP_IZQ como referencia\n    axes[2].imshow(sample[0, 0].cpu(), cmap='gray')\n    axes[2].set_title(f'Original {LIGHT_NAMES[0]}')\n    axes[2].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(SAVE_DIR, 'attention_analysis.png'), dpi=150)\n    plt.show()"
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}