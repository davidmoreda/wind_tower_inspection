{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE-TSCM + VQVAE v6.1 — Fix FP32 en VQ + Gradient Clipping\n",
    "\n",
    "Pipeline de fusión multi-iluminación para detección de defectos superficiales.\n",
    "\n",
    "**Pipeline:** 5 luces (SUP_IZQ, SUP_DER, INF_DER, INF_IZQ, ALL) → AE-TSCM → RGB → VQVAE → 5 luces reconstruidas\n",
    "\n",
    "**Fixes vs v6.0 (que producía NaN en todas las épocas):**\n",
    "- **FIX CRITICO: FP32 forzado en VectorQuantizerEMA** — `autocast(enabled=False)` dentro del VQ. Con imágenes 5472x3072, el mapa latente tiene 65,664 posiciones; las sumas en EMA update desbordaban FP16 (max 65,504) → inf → NaN\n",
    "- **BatchNorm pre-VQ** — normaliza salida del encoder (~N(0,1)) para distancias estables al codebook\n",
    "- **Codebook init Normal(0, 0.5)** — matching con distribución BatchNorm, en vez de Uniform(-1/N, 1/N) que era demasiado pequeña\n",
    "- **Epsilon FP16-safe** — ChannelNormalize usa 1e-4 en vez de 1e-8 (que es 0 en FP16)\n",
    "- **Gradient clipping** — `clip_grad_norm_(max_norm=1.0)` con `scaler.unscale_()` previo\n",
    "- **NaN early stopping** — detiene tras 5 épocas consecutivas con NaN\n",
    "\n",
    "**Configuración (sin cambios):**\n",
    "- Mayor capacidad: `hidden_channels=128`, `num_residual=4` (~13M params)\n",
    "- Codebook EMA con reset periódico, 128 entradas × 128d\n",
    "- Resolución completa sin resize (5472×3072)\n",
    "- Mixed precision (AMP) + gradient accumulation\n",
    "- Requiere A100 (40GB+)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-msssim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n",
      "GPU: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "# NO importamos scheduler — el paper usa LR constante\n",
    "from torch.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Dispositivo: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Montar Google Drive y configurar rutas\n",
    "\n",
    "Sube la carpeta `MT001/` a tu Google Drive, por ejemplo en `MyDrive/dataset/MT001/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# AJUSTA ESTA RUTA a donde subiste la carpeta MT001\n",
    "# ============================================================\n",
    "DATASET_DIR = 'dataset/dataset/MT001'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados se guardarán en: runs/ae_tscm_v6_20260128_110647\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    # Datos\n",
    "    'data_dir': DATASET_DIR,\n",
    "    'num_lights': 5,              # SUP_IZQ, SUP_DER, INF_DER, INF_IZQ, ALL\n",
    "    'batch_size': 1,              # Batch size 1 para resolución completa\n",
    "    'gradient_accumulation_steps': 4,  # Simula batch efectivo de 4\n",
    "\n",
    "    # Modelo AE-TSCM\n",
    "    'use_spatial_attention': True,\n",
    "\n",
    "    # Modelo VQVAE — mayor capacidad para A100\n",
    "    'hidden_channels': 128,       # 128 vs 64 en v5 → más capacidad en encoder/decoder\n",
    "    'num_residual': 4,            # 4 bloques residuales vs 2 en v5\n",
    "    'num_embeddings': 128,        # 256 vs 512 → codebook más pequeño, más fácil de utilizar\n",
    "    'embedding_dim': 128,         # 128 vs 64 → latent más expresivo\n",
    "\n",
    "    # Codebook EMA\n",
    "    'ema_decay': 0.99,            # Factor de decaimiento para EMA\n",
    "    'commitment_cost': 0.25,      # Peso del commitment loss\n",
    "    'codebook_reset_interval': 10,  # Reset entradas no usadas cada N épocas\n",
    "    'codebook_usage_threshold': 1,  # Mínimo de usos para no ser reseteada\n",
    "\n",
    "    # Entrenamiento\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 1e-3,        # LR constante, sin scheduler\n",
    "    'weight_decay': 1e-5,\n",
    "\n",
    "    # Pesos de pérdidas\n",
    "    'lambda_mse': 1.0,\n",
    "    'lambda_ssim': 0.05,\n",
    "    'lambda_vq': 0.02,\n",
    "\n",
    "    # Guardado\n",
    "    'save_interval': 25,\n",
    "    'log_interval': 10,\n",
    "}\n",
    "\n",
    "# Directorio de salida en Drive para no perder resultados\n",
    "SAVE_DIR = 'runs/ae_tscm_v6_' + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, 'visualizations'), exist_ok=True)\n",
    "print(f'Resultados se guardarán en: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset — Loader para formato MT001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orden fijo de las luces\n",
    "LIGHT_NAMES = ['SUP_IZQ', 'SUP_DER', 'INF_DER', 'INF_IZQ', 'ALL']\n",
    "\n",
    "\n",
    "class CropToDivisible:\n",
    "    \"\"\"Recorta la imagen para que H y W sean divisibles por un factor (16 para 4 etapas de stride 2).\"\"\"\n",
    "    def __init__(self, factor=16):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        _, h, w = img.shape\n",
    "        new_h = (h // self.factor) * self.factor\n",
    "        new_w = (w // self.factor) * self.factor\n",
    "        top = (h - new_h) // 2\n",
    "        left = (w - new_w) // 2\n",
    "        return img[:, top:top + new_h, left:left + new_w]\n",
    "\n",
    "\n",
    "class MT001Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para imágenes MT001 con 5 iluminaciones.\n",
    "    Agrupa por num_captura y devuelve (5, H, W) en escala de grises.\n",
    "    Trabaja a resolución completa (sin resize).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, light_names=LIGHT_NAMES, transform=None, captures=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.light_names = light_names\n",
    "        self.num_lights = len(light_names)\n",
    "        self.transform = transform\n",
    "        self.groups = self._parse_metadata(captures)\n",
    "        print(f'  Dataset: {len(self.groups)} capturas, {self.num_lights} luces cada una')\n",
    "\n",
    "    def _parse_metadata(self, captures):\n",
    "        metadata_path = os.path.join(self.root_dir, 'metadata.csv')\n",
    "        groups = defaultdict(dict)\n",
    "\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                cap = int(row['num_captura'])\n",
    "                luz = row['luz']\n",
    "                if luz in self.light_names:\n",
    "                    groups[cap][luz] = row['imagen']\n",
    "\n",
    "        valid = []\n",
    "        for cap in sorted(groups.keys()):\n",
    "            if captures is not None and cap not in captures:\n",
    "                continue\n",
    "            if len(groups[cap]) == self.num_lights:\n",
    "                valid.append((cap, groups[cap]))\n",
    "\n",
    "        return valid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap_num, filenames = self.groups[idx]\n",
    "        channels = []\n",
    "\n",
    "        for light_name in self.light_names:\n",
    "            img_path = os.path.join(self.root_dir, filenames[light_name])\n",
    "            image = Image.open(img_path).convert('L')  # Escala de grises\n",
    "            image = transforms.ToTensor()(image)  # (1, H, W), rango [0,1]\n",
    "            channels.append(image)\n",
    "\n",
    "        multi_channel = torch.cat(channels, dim=0)  # (5, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            multi_channel = self.transform(multi_channel)\n",
    "\n",
    "        return multi_channel\n",
    "\n",
    "\n",
    "def get_train_transforms():\n",
    "    \"\"\"Transforms para entrenamiento a resolución completa.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        CropToDivisible(16),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"Transforms para validación a resolución completa.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        CropToDivisible(16),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset: 62 capturas, 5 luces cada una\n",
      "\n",
      "Forma muestra: torch.Size([5, 3072, 5472])\n",
      "Rango valores: [0.000, 1.000]\n",
      "Total capturas para entrenamiento: 62\n",
      "\n",
      "Memoria GPU:\n",
      "  Asignada: 1.47 GB\n",
      "  Reservada: 17.88 GB\n",
      "  Disponible: 50.06 GB\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset con TODAS las capturas (sin split, como el paper)\n",
    "train_dataset = MT001Dataset(\n",
    "    CONFIG['data_dir'],\n",
    "    transform=get_train_transforms(),\n",
    ")\n",
    "\n",
    "# OPTIMIZACIÓN: pin_memory=False para evitar OOM, num_workers reducido\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=0,         # Reducido a 0 para minimizar uso de memoria\n",
    "    pin_memory=False,      # Desactivado para evitar OOM en GPU\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "# Verificar una muestra\n",
    "sample = train_dataset[0]\n",
    "print(f'\\nForma muestra: {sample.shape}')  # (5, H, W) a resolución completa\n",
    "print(f'Rango valores: [{sample.min():.3f}, {sample.max():.3f}]')\n",
    "print(f'Total capturas para entrenamiento: {len(train_dataset)}')\n",
    "\n",
    "# Verificar memoria disponible\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\\nMemoria GPU:')\n",
    "    print(f'  Asignada: {torch.cuda.memory_allocated(0)/1e9:.2f} GB')\n",
    "    print(f'  Reservada: {torch.cuda.memory_reserved(0)/1e9:.2f} GB')\n",
    "    print(f'  Disponible: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AE-TSCM: Attention-Enhanced Taylor Series Channel Mixer\n",
    "# ============================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"SE-Net: aprende qué luces son más importantes.\"\"\"\n",
    "    def __init__(self, num_channels, reduction=2):\n",
    "        super().__init__()\n",
    "        hidden = max(num_channels // reduction, 4)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(num_channels, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        gap = x.mean(dim=[2, 3])\n",
    "        weights = self.attention(gap)\n",
    "        return weights.view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"CBAM: aprende qué regiones espaciales son más importantes.\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = x.mean(dim=1, keepdim=True)\n",
    "        max_out, _ = x.max(dim=1, keepdim=True)\n",
    "        return self.conv(torch.cat([avg_out, max_out], dim=1))\n",
    "\n",
    "\n",
    "class TaylorTransform(nn.Module):\n",
    "    \"\"\"Transformación no lineal de intensidad con coeficientes aprendibles.\"\"\"\n",
    "    def __init__(self, num_coefficients=5):\n",
    "        super().__init__()\n",
    "        initial = torch.zeros(num_coefficients)\n",
    "        initial[1] = 1.0  # f(x) = x inicialmente\n",
    "        self.coefficients = nn.Parameter(initial)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.coefficients\n",
    "        return (c[0] + c[1]*x + c[2]*(x**2)/2 + c[3]*(x**3)/6 + c[4]*(x**4)/24)\n",
    "\n",
    "\n",
    "class ChannelMixer(nn.Module):\n",
    "    \"\"\"Mezcla N canales a M canales con conv 1x1.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mixer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mixer(x)\n",
    "\n",
    "\n",
    "class ChannelNormalize(nn.Module):\n",
    "    \"\"\"Normaliza cada canal a [0, 1].\"\"\"\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x_flat = x.view(b, c, -1)\n",
    "        mn = x_flat.min(dim=2, keepdim=True)[0].unsqueeze(-1)\n",
    "        mx = x_flat.max(dim=2, keepdim=True)[0].unsqueeze(-1)\n",
    "        # FIX: epsilon 1e-4 en vez de 1e-8 (1e-8 es cero en FP16, min subnormal ~ 6e-8)\n",
    "        return (x - mn) / (mx - mn + 1e-4)\n",
    "\n",
    "\n",
    "class AE_TSCM(nn.Module):\n",
    "    \"\"\"5 luces → 3 canales RGB.\"\"\"\n",
    "    def __init__(self, in_channels=5, out_channels=3, use_spatial_attention=True):\n",
    "        super().__init__()\n",
    "        self.use_spatial_attention = use_spatial_attention\n",
    "        self.taylor = TaylorTransform(5)\n",
    "        self.channel_attention = ChannelAttention(in_channels)\n",
    "        if use_spatial_attention:\n",
    "            self.spatial_attention = SpatialAttention(7)\n",
    "        self.mixer = ChannelMixer(in_channels, out_channels)\n",
    "        self.normalize = ChannelNormalize()\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        x = self.taylor(x)\n",
    "        ch_w = self.channel_attention(x)\n",
    "        x = x * ch_w\n",
    "        sp_w = None\n",
    "        if self.use_spatial_attention:\n",
    "            sp_w = self.spatial_attention(x)\n",
    "            x = x * sp_w\n",
    "        x = self.mixer(x)\n",
    "        x = self.normalize(x)\n",
    "        if return_attention:\n",
    "            return x, ch_w.squeeze(), sp_w\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VQVAE: Vector Quantized VAE — 4 etapas (16x down/up)\n",
    "# Con EMA codebook + reset de entradas no usadas\n",
    "#\n",
    "# FIX CRITICO v7: FP32 forzado en VQ para evitar overflow\n",
    "# Con imágenes 5472x3072, el mapa latente tiene 342x192 = 65,664 posiciones\n",
    "# FP16 max = 65,504 → sumas en EMA update desbordan en FP16\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"4 etapas de downsampling (16x) para resolución completa.\n",
    "    FIX: BatchNorm al final para normalizar salida antes del VQ.\"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_channels=128, embedding_dim=128, num_residual=4):\n",
    "        super().__init__()\n",
    "        hc = hidden_channels\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hc, 4, stride=2, padding=1), nn.ReLU())       # /2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hc, hc, 4, stride=2, padding=1), nn.ReLU())                # /4\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hc, hc * 2, 4, stride=2, padding=1), nn.ReLU())            # /8\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hc * 2, hc * 2, 4, stride=2, padding=1), nn.ReLU())        # /16\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hc * 2) for _ in range(num_residual)])\n",
    "        # FIX: BatchNorm normaliza la salida del encoder (~N(0,1)) para que\n",
    "        # las distancias al codebook sean estables y la inicialización funcione\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Conv2d(hc * 2, embedding_dim, 1),\n",
    "            nn.BatchNorm2d(embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        return self.to_latent(x)\n",
    "\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"\n",
    "    VQ con actualización EMA del codebook.\n",
    "\n",
    "    FIX CRITICO: Todas las operaciones internas se fuerzan a FP32\n",
    "    usando autocast(enabled=False). Sin esto, las sumas sobre >65k\n",
    "    posiciones espaciales desbordan FP16 (max 65504) produciendo\n",
    "    inf → NaN que corrompe todo el entrenamiento.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=256, embedding_dim=128,\n",
    "                 commitment_cost=0.25, decay=0.99, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Codebook — Normal(0, 0.5) para matching con BatchNorm del encoder (~N(0,1))\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.codebook.weight.data.normal_(0, 0.5)\n",
    "        self.codebook.weight.requires_grad = False  # EMA actualiza, no gradientes\n",
    "\n",
    "        # Buffers EMA (no son parámetros del optimizador)\n",
    "        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self.register_buffer('ema_dw', self.codebook.weight.data.clone())\n",
    "        self.register_buffer('usage_count', torch.zeros(num_embeddings))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # ============================================================\n",
    "        # FIX CRITICO: Deshabilitar autocast y forzar FP32\n",
    "        # Sin esto, encodings.t() @ z_flat (65664 x 128) desborda FP16\n",
    "        # ============================================================\n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            z = z.float()\n",
    "\n",
    "            z = z.permute(0, 2, 3, 1).contiguous()\n",
    "            z_shape = z.shape\n",
    "            z_flat = z.view(-1, self.embedding_dim)\n",
    "\n",
    "            # Distancias al codebook (FP32 seguro)\n",
    "            distances = (\n",
    "                torch.sum(z_flat**2, dim=1, keepdim=True) +\n",
    "                torch.sum(self.codebook.weight**2, dim=1) -\n",
    "                2 * torch.matmul(z_flat, self.codebook.weight.t())\n",
    "            )\n",
    "\n",
    "            encoding_indices = torch.argmin(distances, dim=1)\n",
    "            encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
    "            quantized = self.codebook(encoding_indices).view(z_shape)\n",
    "\n",
    "            # EMA update del codebook (solo en training, todo en FP32)\n",
    "            if self.training:\n",
    "                encodings_sum = encodings.sum(0)\n",
    "                dw = encodings.t() @ z_flat  # FP32: seguro para >65k posiciones\n",
    "\n",
    "                self.ema_cluster_size.data.mul_(self.decay).add_(\n",
    "                    encodings_sum, alpha=1 - self.decay)\n",
    "                self.ema_dw.data.mul_(self.decay).add_(\n",
    "                    dw, alpha=1 - self.decay)\n",
    "\n",
    "                # Laplace smoothing para evitar divisiones por cero\n",
    "                n = self.ema_cluster_size.sum()\n",
    "                cluster_size = (\n",
    "                    (self.ema_cluster_size + self.epsilon) /\n",
    "                    (n + self.num_embeddings * self.epsilon) * n\n",
    "                )\n",
    "\n",
    "                self.codebook.weight.data.copy_(\n",
    "                    self.ema_dw / cluster_size.unsqueeze(1))\n",
    "\n",
    "                # Acumular uso para reset\n",
    "                self.usage_count.add_(encodings_sum)\n",
    "\n",
    "            # Solo commitment loss (codebook se actualiza por EMA)\n",
    "            commitment_loss = F.mse_loss(z, quantized.detach())\n",
    "            loss = self.commitment_cost * commitment_loss\n",
    "\n",
    "            # Straight-through estimator\n",
    "            quantized = z + (quantized - z).detach()\n",
    "\n",
    "            # Perplexity (mide utilización del codebook)\n",
    "            avg_probs = encodings.mean(0)\n",
    "            perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "            quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "            return quantized, loss, perplexity\n",
    "\n",
    "    def reset_unused_codes(self, z_flat_samples, threshold=2):\n",
    "        \"\"\"\n",
    "        Reemplaza entradas del codebook con uso < threshold\n",
    "        por vectores aleatorios del encoder (z_flat_samples).\n",
    "        \"\"\"\n",
    "        z_flat_samples = z_flat_samples.float()  # FIX: asegurar FP32\n",
    "        unused_mask = self.usage_count < threshold\n",
    "        num_unused = unused_mask.sum().item()\n",
    "\n",
    "        if num_unused > 0 and z_flat_samples.shape[0] > 0:\n",
    "            n_replace = min(int(num_unused), z_flat_samples.shape[0])\n",
    "            # Muestrear vectores del encoder para reemplazar entradas muertas\n",
    "            perm = torch.randperm(z_flat_samples.shape[0], device=z_flat_samples.device)\n",
    "            replace_vectors = z_flat_samples[perm[:n_replace]].detach()\n",
    "\n",
    "            unused_indices = torch.where(unused_mask)[0][:n_replace]\n",
    "            self.codebook.weight.data[unused_indices] = replace_vectors\n",
    "            self.ema_dw.data[unused_indices] = replace_vectors\n",
    "            self.ema_cluster_size.data[unused_indices] = 1.0\n",
    "\n",
    "        # Reset contador de uso para el siguiente periodo\n",
    "        self.usage_count.zero_()\n",
    "        return num_unused\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"4 etapas de upsampling (16x) para resolución completa.\"\"\"\n",
    "    def __init__(self, out_channels=5, hidden_channels=128, embedding_dim=128, num_residual=4):\n",
    "        super().__init__()\n",
    "        hc = hidden_channels\n",
    "        self.from_latent = nn.Conv2d(embedding_dim, hc * 2, 1)\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hc * 2) for _ in range(num_residual)])\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hc * 2, hc * 2, 4, stride=2, padding=1), nn.ReLU())   # x2\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hc * 2, hc, 4, stride=2, padding=1), nn.ReLU())       # x4\n",
    "        self.deconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hc, hc, 4, stride=2, padding=1), nn.ReLU())           # x8\n",
    "        self.deconv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hc, out_channels, 4, stride=2, padding=1), nn.Sigmoid())  # x16\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.from_latent(z)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        return self.deconv4(x)\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=5, hidden_channels=128,\n",
    "                 num_residual=4, num_embeddings=256, embedding_dim=128,\n",
    "                 commitment_cost=0.25, ema_decay=0.99):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, embedding_dim, num_residual)\n",
    "        self.vq = VectorQuantizerEMA(num_embeddings, embedding_dim, commitment_cost, ema_decay)\n",
    "        self.decoder = Decoder(out_channels, hidden_channels, embedding_dim, num_residual)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, vq_loss, perplexity = self.vq(z)\n",
    "        return self.decoder(z_q), vq_loss, perplexity\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, _, _ = self.vq(z)\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros totales: 13,211,702\n",
      "Parámetros entrenables: 13,195,318  (codebook se actualiza por EMA)\n",
      "Codebook: 128 entradas × 128d\n",
      "\n",
      "Memoria GPU después de cargar modelo:\n",
      "  Asignada: 1.52 GB\n",
      "  Reservada: 17.88 GB\n",
      "  Disponible: 50.01 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Modelo Completo: AE-TSCM + VQVAE\n",
    "# ============================================================\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    \"\"\"5 luces → AE-TSCM → RGB → VQVAE → 5 luces reconstruidas\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ae_tscm = AE_TSCM(\n",
    "            in_channels=config['num_lights'],\n",
    "            out_channels=3,\n",
    "            use_spatial_attention=config['use_spatial_attention']\n",
    "        )\n",
    "        self.vqvae = VQVAE(\n",
    "            in_channels=3,\n",
    "            out_channels=config['num_lights'],\n",
    "            hidden_channels=config['hidden_channels'],\n",
    "            num_residual=config['num_residual'],\n",
    "            num_embeddings=config['num_embeddings'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            commitment_cost=config['commitment_cost'],\n",
    "            ema_decay=config['ema_decay']\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        if return_intermediate:\n",
    "            rgb, ch_att, sp_att = self.ae_tscm(x, return_attention=True)\n",
    "        else:\n",
    "            rgb = self.ae_tscm(x, return_attention=False)\n",
    "        reconstructed, vq_loss, perplexity = self.vqvae(rgb)\n",
    "        if return_intermediate:\n",
    "            return reconstructed, vq_loss, perplexity, rgb, ch_att, sp_att\n",
    "        return reconstructed, vq_loss, perplexity\n",
    "\n",
    "\n",
    "# Crear modelo y mostrar parámetros\n",
    "model = FullModel(CONFIG).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Parámetros totales: {total_params:,}')\n",
    "print(f'Parámetros entrenables: {trainable_params:,}  (codebook se actualiza por EMA)')\n",
    "print(f'Codebook: {CONFIG[\"num_embeddings\"]} entradas × {CONFIG[\"embedding_dim\"]}d')\n",
    "\n",
    "# Verificar memoria después de crear modelo\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\\nMemoria GPU después de cargar modelo:')\n",
    "    print(f'  Asignada: {torch.cuda.memory_allocated(0)/1e9:.2f} GB')\n",
    "    print(f'  Reservada: {torch.cuda.memory_reserved(0)/1e9:.2f} GB')\n",
    "    print(f'  Disponible: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funciones de pérdida y visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(original, reconstructed, vq_loss, config):\n",
    "    mse_loss = F.mse_loss(reconstructed, original)\n",
    "    ssim_loss = 1 - ssim(reconstructed, original, data_range=1.0, size_average=True)\n",
    "    total = (config['lambda_mse'] * mse_loss +\n",
    "             config['lambda_ssim'] * ssim_loss +\n",
    "             config['lambda_vq'] * vq_loss)\n",
    "    return total, {\n",
    "        'total': total.item(),\n",
    "        'mse': mse_loss.item(),\n",
    "        'ssim': ssim_loss.item(),\n",
    "        'vq': vq_loss.item()\n",
    "    }\n",
    "\n",
    "\n",
    "def save_visualization(original, reconstructed, rgb, epoch, save_dir):\n",
    "    original = original[0].detach().cpu()\n",
    "    reconstructed = reconstructed[0].detach().cpu()\n",
    "    rgb = rgb[0].detach().cpu()\n",
    "    num_lights = original.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(3, max(num_lights, 3), figsize=(3*num_lights, 9))\n",
    "\n",
    "    for i in range(num_lights):\n",
    "        axes[0, i].imshow(original[i], cmap='gray', vmin=0, vmax=1)\n",
    "        axes[0, i].set_title(f'Orig {LIGHT_NAMES[i]}', fontsize=8)\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(reconstructed[i], cmap='gray', vmin=0, vmax=1)\n",
    "        axes[1, i].set_title(f'Recon {LIGHT_NAMES[i]}', fontsize=8)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    rgb_img = rgb.permute(1, 2, 0).numpy()\n",
    "    rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min() + 1e-8)\n",
    "    axes[2, 0].imshow(rgb_img)\n",
    "    axes[2, 0].set_title('RGB Fusionado')\n",
    "    axes[2, 0].axis('off')\n",
    "    for i in range(1, max(num_lights, 3)):\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "    plt.suptitle(f'Época {epoch}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:04d}.png'), dpi=100)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_losses(history, save_dir):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    epochs = range(1, len(history['total']) + 1)\n",
    "\n",
    "    for ax, key, title in zip(\n",
    "        axes.flat[:4],\n",
    "        ['total', 'mse', 'ssim', 'vq'],\n",
    "        ['Total Loss', 'MSE Loss', 'SSIM Loss', 'VQ Loss']\n",
    "    ):\n",
    "        ax.plot(epochs, history[key], label='train')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    # Perplexity (utilización del codebook)\n",
    "    ax_perp = axes[1, 1]\n",
    "    ax_perp.plot(epochs, history['perplexity'], label='perplexity', color='green')\n",
    "    ax_perp.axhline(y=CONFIG['num_embeddings'], color='r', linestyle='--', alpha=0.5,\n",
    "                    label=f'max ({CONFIG[\"num_embeddings\"]})')\n",
    "    ax_perp.set_title('Codebook Perplexity')\n",
    "    ax_perp.set_xlabel('Epoch')\n",
    "    ax_perp.legend()\n",
    "\n",
    "    # SSIM value (1 - ssim_loss)\n",
    "    ax_ssim_val = axes[1, 2]\n",
    "    ssim_values = [1 - s for s in history['ssim']]\n",
    "    ax_ssim_val.plot(epochs, ssim_values, label='SSIM', color='purple')\n",
    "    ax_ssim_val.set_title('SSIM (calidad reconstrucción)')\n",
    "    ax_ssim_val.set_xlabel('Epoch')\n",
    "    ax_ssim_val.set_ylim(0, 1)\n",
    "    ax_ssim_val.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'loss_history.png'), dpi=150)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def run_epoch(model, dataloader, optimizer, config, scaler=None, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_losses = {'total': 0, 'mse': 0, 'ssim': 0, 'vq': 0}\n",
    "    total_perplexity = 0\n",
    "    num_batches = 0\n",
    "    nan_batches = 0\n",
    "    accum_steps = config.get('gradient_accumulation_steps', 1)\n",
    "    use_amp = scaler is not None\n",
    "\n",
    "    ctx = torch.no_grad() if not train else torch.enable_grad()\n",
    "    with ctx:\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc='Train', leave=False)):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            with autocast('cuda', enabled=use_amp):\n",
    "                reconstructed, vq_loss, perplexity = model(batch)\n",
    "                loss, loss_dict = compute_loss(batch, reconstructed, vq_loss, config)\n",
    "                if train:\n",
    "                    loss = loss / accum_steps\n",
    "\n",
    "            # FIX: Detectar NaN/Inf y saltar el batch en vez de corromper gradientes\n",
    "            if not torch.isfinite(loss):\n",
    "                nan_batches += 1\n",
    "                if train:\n",
    "                    optimizer.zero_grad()  # Limpiar gradientes corruptos\n",
    "                continue\n",
    "\n",
    "            if train:\n",
    "                if use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    if (i + 1) % accum_steps == 0:\n",
    "                        # FIX: Unscale + gradient clipping antes del step\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    if (i + 1) % accum_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "            for k in total_losses:\n",
    "                total_losses[k] += loss_dict[k]\n",
    "            total_perplexity += perplexity.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # Flush gradientes restantes si no son divisibles por accum_steps\n",
    "        if train and num_batches % accum_steps != 0:\n",
    "            if use_amp:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    for k in total_losses:\n",
    "        total_losses[k] /= max(num_batches, 1)\n",
    "    avg_perplexity = total_perplexity / max(num_batches, 1)\n",
    "\n",
    "    if nan_batches > 0:\n",
    "        print(f'  >> AVISO: {nan_batches} batches con NaN/Inf saltados')\n",
    "\n",
    "    return total_losses, avg_perplexity\n",
    "\n",
    "\n",
    "def collect_encoder_outputs(model, dataloader, max_samples=8):\n",
    "    \"\"\"Recoge outputs del encoder para usar como reemplazos en codebook reset.\"\"\"\n",
    "    model.eval()\n",
    "    z_samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= max_samples:\n",
    "                break\n",
    "            batch = batch.to(device)\n",
    "            with autocast('cuda'):\n",
    "                rgb = model.ae_tscm(batch)\n",
    "                z = model.vqvae.encoder(rgb)\n",
    "            # Flatten spatial dims: (B, D, H, W) → (B*H*W, D)\n",
    "            z_flat = z.permute(0, 2, 3, 1).contiguous().view(-1, z.shape[1])\n",
    "            z_samples.append(z_flat.float())  # FIX: asegurar FP32\n",
    "    if z_samples:\n",
    "        return torch.cat(z_samples, dim=0)\n",
    "    return torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> AVISO: 62 batches con NaN/Inf saltados\n",
      "Ep  12/150 | Loss: 0.0000 (mse:0.0000 ssim:0.0000 vq:0.0000) | Perp: 0.0/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m train_losses, avg_perplexity = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# FIX: Detección temprana de NaN para no desperdiciar 150 épocas\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m math.isnan(train_losses[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m]):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, config, scaler, train)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[32m     19\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mMT001Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m light_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.light_names:\n\u001b[32m     63\u001b[39m     img_path = os.path.join(\u001b[38;5;28mself\u001b[39m.root_dir, filenames[light_name])\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Escala de grises\u001b[39;00m\n\u001b[32m     65\u001b[39m     image = transforms.ToTensor()(image)  \u001b[38;5;66;03m# (1, H, W), rango [0,1]\u001b[39;00m\n\u001b[32m     66\u001b[39m     channels.append(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/PIL/Image.py:967\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    916\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    917\u001b[39m     mode: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    921\u001b[39m     colors: \u001b[38;5;28mint\u001b[39m = \u001b[32m256\u001b[39m,\n\u001b[32m    922\u001b[39m ) -> Image:\n\u001b[32m    923\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[33;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[33;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    964\u001b[39m \u001b[33;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[32m    965\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m     has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    971\u001b[39m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wind_tower_inspection/gri_env/.venv/lib/python3.11/site-packages/PIL/ImageFile.py:406\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    403\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    405\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Optimizador y scaler (AMP) — SIN SCHEDULER, LR constante\n",
    "# ============================================================\n",
    "optimizer = Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Historial\n",
    "history = {k: [] for k in ['total', 'mse', 'ssim', 'vq', 'perplexity']}\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f'Iniciando entrenamiento: {CONFIG[\"epochs\"]} épocas')\n",
    "print(f'Muestras: {len(train_dataset)} (todas, sin split)')\n",
    "print(f'Batch size: {CONFIG[\"batch_size\"]} | Gradient accumulation: {CONFIG[\"gradient_accumulation_steps\"]} (batch efectivo: {CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]})')\n",
    "print(f'Learning rate: {CONFIG[\"learning_rate\"]} (CONSTANTE, sin scheduler)')\n",
    "print(f'Mixed precision (AMP): activado | VQ forzado a FP32')\n",
    "print(f'Codebook: {CONFIG[\"num_embeddings\"]} entradas, EMA decay={CONFIG[\"ema_decay\"]}, reset cada {CONFIG[\"codebook_reset_interval\"]} épocas')\n",
    "print(f'Gradient clipping: max_norm=1.0')\n",
    "print('=' * 70)\n",
    "\n",
    "nan_streak = 0  # Contador de épocas consecutivas con NaN\n",
    "MAX_NAN_STREAK = 5  # Parar si hay 5 épocas seguidas con NaN\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    # Limpiar caché CUDA al inicio de cada época\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Train\n",
    "    train_losses, avg_perplexity = run_epoch(model, train_loader, optimizer, CONFIG, scaler=scaler, train=True)\n",
    "\n",
    "    # FIX: Detección temprana de NaN para no desperdiciar 150 épocas\n",
    "    if math.isnan(train_losses['total']):\n",
    "        nan_streak += 1\n",
    "        print(f'Ep {epoch:3d}/{CONFIG[\"epochs\"]} | Loss: NaN (racha: {nan_streak}/{MAX_NAN_STREAK})')\n",
    "        if nan_streak >= MAX_NAN_STREAK:\n",
    "            print(f'\\n  ABORT: {MAX_NAN_STREAK} épocas consecutivas con NaN. Revisar modelo/hiperparámetros.')\n",
    "            break\n",
    "        continue\n",
    "    else:\n",
    "        nan_streak = 0  # Reset racha si la época fue válida\n",
    "\n",
    "    # Guardar historial\n",
    "    for k in ['total', 'mse', 'ssim', 'vq']:\n",
    "        history[k].append(train_losses[k])\n",
    "    history['perplexity'].append(avg_perplexity)\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if train_losses['total'] < best_loss:\n",
    "        best_loss = train_losses['total']\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model.pt'))\n",
    "\n",
    "    # Log\n",
    "    print(f'Ep {epoch:3d}/{CONFIG[\"epochs\"]} | '\n",
    "          f'Loss: {train_losses[\"total\"]:.4f} (mse:{train_losses[\"mse\"]:.4f} ssim:{train_losses[\"ssim\"]:.4f} vq:{train_losses[\"vq\"]:.4f}) | '\n",
    "          f'Perp: {avg_perplexity:.1f}/{CONFIG[\"num_embeddings\"]}')\n",
    "\n",
    "    # Mostrar memoria cada 10 épocas\n",
    "    if epoch % 10 == 0 and torch.cuda.is_available():\n",
    "        print(f'  >> Mem: {torch.cuda.memory_allocated(0)/1e9:.2f} GB asignada, '\n",
    "              f'{torch.cuda.max_memory_allocated(0)/1e9:.2f} GB máx')\n",
    "\n",
    "    # Codebook reset — reemplaza entradas no usadas con outputs reales del encoder\n",
    "    if epoch % CONFIG['codebook_reset_interval'] == 0:\n",
    "        z_samples = collect_encoder_outputs(model, train_loader, max_samples=8)\n",
    "        num_reset = model.vqvae.vq.reset_unused_codes(\n",
    "            z_samples, threshold=CONFIG['codebook_usage_threshold'])\n",
    "        usage_pct = (CONFIG['num_embeddings'] - num_reset) / CONFIG['num_embeddings'] * 100\n",
    "        print(f'  >> Codebook reset: {num_reset} entradas reemplazadas, '\n",
    "              f'utilización: {usage_pct:.0f}%')\n",
    "\n",
    "    # Visualización\n",
    "    if epoch % CONFIG['log_interval'] == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample = next(iter(train_loader)).to(device)\n",
    "            with autocast('cuda'):\n",
    "                recon, _, _, rgb, _, _ = model(sample, return_intermediate=True)\n",
    "            save_visualization(sample, recon.float(), rgb.float(), epoch,\n",
    "                               os.path.join(SAVE_DIR, 'visualizations'))\n",
    "\n",
    "    # Checkpoint\n",
    "    if epoch % CONFIG['save_interval'] == 0:\n",
    "        ckpt_path = os.path.join(SAVE_DIR, 'checkpoints', f'ckpt_epoch_{epoch:04d}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'train_loss': train_losses['total'],\n",
    "            'perplexity': avg_perplexity,\n",
    "            'config': CONFIG\n",
    "        }, ckpt_path)\n",
    "        print(f'  >> Checkpoint guardado: epoch {epoch}')\n",
    "\n",
    "# Guardar modelo final y gráficas\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'final_model.pt'))\n",
    "if history['total']:  # Solo graficar si hay datos válidos\n",
    "    plot_losses(history, SAVE_DIR)\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "if history['total']:\n",
    "    print(f'Entrenamiento completado. Loss final: {train_losses[\"total\"]:.4f} | Mejor: {best_loss:.4f}')\n",
    "    print(f'Perplexity final: {avg_perplexity:.1f}/{CONFIG[\"num_embeddings\"]}')\n",
    "else:\n",
    "    print('Entrenamiento fallido: ninguna época produjo loss válido.')\n",
    "print(f'Resultados en: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluación final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo final y visualizar reconstrucciones\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'final_model.pt')))\n",
    "model.eval()\n",
    "\n",
    "print('Visualización de muestras con el modelo final:\\n')\n",
    "\n",
    "# Métricas finales agregadas\n",
    "all_mse = []\n",
    "all_ssim = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        with autocast('cuda'):\n",
    "            recon, vq_loss, perp, rgb, ch_att, sp_att = model(batch, return_intermediate=True)\n",
    "\n",
    "        # Métricas por muestra\n",
    "        mse_val = F.mse_loss(recon.float(), batch).item()\n",
    "        ssim_val = ssim(recon.float(), batch, data_range=1.0, size_average=True).item()\n",
    "        all_mse.append(mse_val)\n",
    "        all_ssim.append(ssim_val)\n",
    "\n",
    "        if i < 3:  # Solo mostrar 3 muestras detalladas\n",
    "            # Channel attention weights\n",
    "            ch_weights = ch_att.float().cpu().numpy().flatten()\n",
    "            print(f'Muestra {i} — MSE: {mse_val:.4f}, SSIM: {ssim_val:.4f}')\n",
    "            print(f'  Channel Attention weights:')\n",
    "            for j, name in enumerate(LIGHT_NAMES):\n",
    "                bar = '█' * int(ch_weights[j] * 30)\n",
    "                print(f'    {name:8s}: {ch_weights[j]:.3f} {bar}')\n",
    "            print()\n",
    "\n",
    "            # Visualizar\n",
    "            orig = batch[0].cpu()\n",
    "            rec = recon[0].float().cpu()\n",
    "\n",
    "            fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "            for j in range(5):\n",
    "                axes[0, j].imshow(orig[j], cmap='gray', vmin=0, vmax=1)\n",
    "                axes[0, j].set_title(f'Orig {LIGHT_NAMES[j]}', fontsize=9)\n",
    "                axes[0, j].axis('off')\n",
    "                axes[1, j].imshow(rec[j], cmap='gray', vmin=0, vmax=1)\n",
    "                axes[1, j].set_title(f'Recon {LIGHT_NAMES[j]}', fontsize=9)\n",
    "                axes[1, j].axis('off')\n",
    "            plt.suptitle(f'Muestra {i} — MSE: {mse_val:.4f}, SSIM: {ssim_val:.4f}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Resumen\n",
    "print('=' * 50)\n",
    "print(f'Métricas finales sobre {len(all_mse)} muestras:')\n",
    "print(f'  MSE  medio: {sum(all_mse)/len(all_mse):.4f}')\n",
    "print(f'  SSIM medio: {sum(all_ssim)/len(all_ssim):.4f}')\n",
    "print(f'  Perplexity final: {history[\"perplexity\"][-1]:.1f}/{CONFIG[\"num_embeddings\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar spatial attention del modelo final\n",
    "with torch.no_grad():\n",
    "    sample = next(iter(train_loader)).to(device)\n",
    "    with autocast('cuda'):\n",
    "        _, _, _, rgb, _, sp_att = model(sample, return_intermediate=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Spatial attention map\n",
    "    sp_map = sp_att[0, 0].float().cpu().numpy()\n",
    "    axes[0].imshow(sp_map, cmap='hot')\n",
    "    axes[0].set_title('Spatial Attention (zonas importantes)')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # RGB fusionado\n",
    "    rgb_np = rgb[0].float().cpu().permute(1, 2, 0).numpy()\n",
    "    rgb_np = (rgb_np - rgb_np.min()) / (rgb_np.max() - rgb_np.min() + 1e-8)\n",
    "    axes[1].imshow(rgb_np)\n",
    "    axes[1].set_title('RGB Fusionado')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Original SUP_IZQ como referencia\n",
    "    axes[2].imshow(sample[0, 0].cpu(), cmap='gray')\n",
    "    axes[2].set_title(f'Original {LIGHT_NAMES[0]}')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'attention_analysis.png'), dpi=150)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
